{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee0fccd470>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = x.astype(np.float32)\n",
    "    x /=255\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size = len(x)\n",
    "    one_hot = np.zeros((batch_size,10))\n",
    "    one_hot[np.arange(batch_size), x] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, (None, *image_shape), name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.int32, (None, n_classes), name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None), name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    conv_num_inputs = int(x_tensor.shape[3])\n",
    "    weights_shape = (*conv_ksize, conv_num_inputs, conv_num_outputs)\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=(weights_shape), mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    strides = [1, *conv_strides, 1]\n",
    "    x = tf.nn.conv2d(x_tensor, weights, strides, padding='SAME')\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    x = tf.nn.elu(x)\n",
    "    x = tf.nn.max_pool(x, [1, *pool_ksize, 1], [1, *pool_strides, 1], \"SAME\")\n",
    "    return x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    weights_shape = (int(x_tensor.shape[1]), num_outputs)\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=weights_shape, mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    x = tf.matmul(x_tensor,weights)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights_shape = (int(x_tensor.shape[1]), num_outputs)\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=weights_shape, mean=0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    x = tf.matmul(x_tensor,weights)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    x = conv2d_maxpool(x, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = conv2d_maxpool(x, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    x = conv2d_maxpool(x, 128, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = conv2d_maxpool(x, 256, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x = fully_conn(x, 512)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    x = fully_conn(x, 256)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    x = output(x, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict= {\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict= {\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    \n",
    "    validation = session.run(accuracy, feed_dict= {\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "\n",
    "    print(\"Loss: {:>10.3f} Validation Accuracy: {:5.2f}%\".format(loss, validation*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:      2.209 Validation Accuracy: 19.98%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:      2.157 Validation Accuracy: 23.92%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:      2.087 Validation Accuracy: 27.26%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:      2.052 Validation Accuracy: 28.40%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:      1.964 Validation Accuracy: 29.50%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:      1.958 Validation Accuracy: 29.66%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:      1.974 Validation Accuracy: 29.00%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:      1.868 Validation Accuracy: 32.68%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:      1.854 Validation Accuracy: 33.84%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:      1.792 Validation Accuracy: 37.46%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:      1.864 Validation Accuracy: 34.50%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:      1.735 Validation Accuracy: 38.92%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:      1.713 Validation Accuracy: 38.14%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:      1.840 Validation Accuracy: 36.26%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:      1.831 Validation Accuracy: 35.88%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:      1.665 Validation Accuracy: 40.02%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:      1.659 Validation Accuracy: 40.06%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:      1.436 Validation Accuracy: 43.26%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:      1.537 Validation Accuracy: 41.20%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:      1.371 Validation Accuracy: 44.60%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:      1.650 Validation Accuracy: 38.62%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:      1.413 Validation Accuracy: 43.18%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:      1.386 Validation Accuracy: 44.26%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:      1.301 Validation Accuracy: 45.80%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:      1.412 Validation Accuracy: 42.72%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:      1.157 Validation Accuracy: 48.56%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:      1.294 Validation Accuracy: 45.54%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:      1.161 Validation Accuracy: 47.46%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:      1.346 Validation Accuracy: 44.06%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:      1.194 Validation Accuracy: 45.94%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:      1.076 Validation Accuracy: 48.06%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:      1.076 Validation Accuracy: 47.62%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:      1.193 Validation Accuracy: 45.44%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:      0.983 Validation Accuracy: 50.38%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:      1.067 Validation Accuracy: 48.00%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:      0.958 Validation Accuracy: 49.82%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:      0.864 Validation Accuracy: 52.92%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:      0.819 Validation Accuracy: 51.90%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:      0.801 Validation Accuracy: 51.98%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:      0.914 Validation Accuracy: 50.56%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:      0.766 Validation Accuracy: 53.06%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:      0.849 Validation Accuracy: 50.48%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:      0.939 Validation Accuracy: 47.80%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:      0.847 Validation Accuracy: 51.46%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:      0.833 Validation Accuracy: 51.30%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:      0.608 Validation Accuracy: 55.82%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:      0.766 Validation Accuracy: 53.10%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:      0.637 Validation Accuracy: 53.30%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:      0.716 Validation Accuracy: 52.30%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:      0.634 Validation Accuracy: 54.72%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:      0.685 Validation Accuracy: 55.42%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:      0.517 Validation Accuracy: 59.02%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:      0.567 Validation Accuracy: 56.30%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:      0.501 Validation Accuracy: 57.16%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:      0.666 Validation Accuracy: 54.88%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:      0.654 Validation Accuracy: 54.98%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:      0.490 Validation Accuracy: 58.90%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:      0.532 Validation Accuracy: 57.86%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:      0.516 Validation Accuracy: 56.44%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:      0.454 Validation Accuracy: 58.82%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:      0.444 Validation Accuracy: 58.72%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:      0.385 Validation Accuracy: 58.78%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:      0.402 Validation Accuracy: 60.02%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:      0.354 Validation Accuracy: 61.80%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:      0.316 Validation Accuracy: 62.26%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:      0.317 Validation Accuracy: 62.70%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:      0.396 Validation Accuracy: 56.88%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:      0.404 Validation Accuracy: 58.34%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:      0.358 Validation Accuracy: 60.72%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:      0.346 Validation Accuracy: 60.24%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:      0.301 Validation Accuracy: 61.68%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:      0.259 Validation Accuracy: 61.62%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:      0.284 Validation Accuracy: 61.18%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:      0.306 Validation Accuracy: 61.60%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:      0.230 Validation Accuracy: 63.36%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:      0.264 Validation Accuracy: 60.78%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:      0.268 Validation Accuracy: 61.82%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:      0.240 Validation Accuracy: 63.02%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:      0.230 Validation Accuracy: 64.20%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:      0.184 Validation Accuracy: 64.80%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:      0.190 Validation Accuracy: 64.96%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:      0.232 Validation Accuracy: 62.96%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:      0.197 Validation Accuracy: 64.02%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:      0.204 Validation Accuracy: 62.40%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:      0.182 Validation Accuracy: 64.36%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:      0.248 Validation Accuracy: 61.06%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:      0.178 Validation Accuracy: 61.56%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:      0.115 Validation Accuracy: 64.84%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:      0.125 Validation Accuracy: 65.90%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:      0.152 Validation Accuracy: 65.10%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:      0.109 Validation Accuracy: 65.82%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:      0.120 Validation Accuracy: 64.88%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:      0.079 Validation Accuracy: 67.16%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:      0.102 Validation Accuracy: 65.30%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:      0.086 Validation Accuracy: 66.54%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:      0.080 Validation Accuracy: 66.68%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:      0.062 Validation Accuracy: 68.04%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:      0.061 Validation Accuracy: 67.24%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:      0.103 Validation Accuracy: 65.66%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:      0.050 Validation Accuracy: 68.18%\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:      0.075 Validation Accuracy: 66.08%\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:      0.070 Validation Accuracy: 66.78%\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:      0.051 Validation Accuracy: 67.36%\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:      0.091 Validation Accuracy: 65.02%\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:      0.036 Validation Accuracy: 69.30%\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:      0.051 Validation Accuracy: 68.44%\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:      0.052 Validation Accuracy: 65.70%\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:      0.031 Validation Accuracy: 68.10%\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:      0.028 Validation Accuracy: 67.92%\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:      0.039 Validation Accuracy: 65.82%\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:      0.031 Validation Accuracy: 67.70%\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:      0.027 Validation Accuracy: 66.98%\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:      0.025 Validation Accuracy: 67.52%\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:      0.013 Validation Accuracy: 69.64%\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:      0.011 Validation Accuracy: 69.16%\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:      0.016 Validation Accuracy: 68.78%\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:      0.036 Validation Accuracy: 65.34%\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:      0.022 Validation Accuracy: 67.90%\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:      0.023 Validation Accuracy: 68.56%\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:      0.030 Validation Accuracy: 65.02%\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:      0.026 Validation Accuracy: 66.32%\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:      0.026 Validation Accuracy: 67.16%\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:      0.016 Validation Accuracy: 70.02%\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 69.06%\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:      0.015 Validation Accuracy: 67.80%\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:      0.011 Validation Accuracy: 69.18%\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 70.74%\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 68.34%\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:      0.012 Validation Accuracy: 68.30%\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:      0.011 Validation Accuracy: 66.72%\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 68.78%\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 69.06%\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 68.94%\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 69.00%\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 67.64%\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 69.28%\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 69.28%\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 68.84%\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 71.04%\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 70.26%\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 70.82%\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 67.28%\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 67.54%\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 69.64%\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 69.04%\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 69.64%\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 70.14%\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 68.80%\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 69.06%\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 69.90%\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 70.06%\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 66.52%\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 69.16%\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 70.14%\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 70.74%\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 69.88%\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.36%\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.68%\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 70.00%\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 70.28%\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 69.90%\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.48%\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 69.92%\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 69.82%\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.66%\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.76%\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 69.42%\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.88%\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 70.30%\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.44%\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.86%\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.76%\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 69.70%\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 69.96%\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.10%\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 70.02%\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.54%\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.22%\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:      0.001 Validation Accuracy: 69.12%\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.16%\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.18%\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 72.06%\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.18%\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.74%\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.16%\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.90%\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 69.74%\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.76%\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.78%\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.42%\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.64%\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.28%\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.82%\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.00%\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.86%\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.62%\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 69.84%\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.52%\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 71.68%\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:      0.000 Validation Accuracy: 70.50%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:      2.165 Validation Accuracy: 21.48%\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:      2.160 Validation Accuracy: 25.60%\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:      1.999 Validation Accuracy: 27.68%\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:      1.931 Validation Accuracy: 30.32%\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:      1.978 Validation Accuracy: 30.52%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:      1.930 Validation Accuracy: 31.36%\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:      1.956 Validation Accuracy: 29.34%\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:      1.596 Validation Accuracy: 36.40%\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:      1.657 Validation Accuracy: 36.78%\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:      2.092 Validation Accuracy: 26.50%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:      1.777 Validation Accuracy: 38.80%\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:      1.869 Validation Accuracy: 34.74%\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:      1.409 Validation Accuracy: 40.78%\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:      1.639 Validation Accuracy: 38.86%\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:      1.761 Validation Accuracy: 39.06%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:      1.751 Validation Accuracy: 41.74%\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:      1.701 Validation Accuracy: 42.76%\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:      1.331 Validation Accuracy: 44.08%\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:      2.015 Validation Accuracy: 34.00%\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:      1.902 Validation Accuracy: 37.00%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:      1.576 Validation Accuracy: 44.88%\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:      1.781 Validation Accuracy: 41.40%\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:      1.380 Validation Accuracy: 42.82%\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:      1.637 Validation Accuracy: 42.90%\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:      1.432 Validation Accuracy: 51.02%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:      1.406 Validation Accuracy: 47.72%\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:      1.551 Validation Accuracy: 49.64%\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:      1.224 Validation Accuracy: 45.36%\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:      1.498 Validation Accuracy: 46.46%\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:      1.601 Validation Accuracy: 47.60%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:      1.334 Validation Accuracy: 49.32%\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:      1.541 Validation Accuracy: 46.38%\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:      1.151 Validation Accuracy: 50.32%\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:      1.623 Validation Accuracy: 44.00%\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:      1.463 Validation Accuracy: 52.56%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:      1.360 Validation Accuracy: 49.36%\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:      1.601 Validation Accuracy: 46.80%\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:      1.047 Validation Accuracy: 53.90%\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:      1.610 Validation Accuracy: 47.12%\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:      1.527 Validation Accuracy: 51.24%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:      1.147 Validation Accuracy: 55.50%\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:      1.472 Validation Accuracy: 49.34%\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:      0.964 Validation Accuracy: 55.50%\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:      1.304 Validation Accuracy: 53.76%\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:      1.412 Validation Accuracy: 54.70%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:      1.066 Validation Accuracy: 57.00%\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:      1.157 Validation Accuracy: 53.96%\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:      0.918 Validation Accuracy: 57.52%\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:      1.191 Validation Accuracy: 55.98%\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:      1.379 Validation Accuracy: 54.44%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:      1.017 Validation Accuracy: 59.64%\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:      1.107 Validation Accuracy: 55.58%\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:      0.987 Validation Accuracy: 55.42%\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:      1.240 Validation Accuracy: 54.32%\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:      1.250 Validation Accuracy: 54.74%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:      0.919 Validation Accuracy: 62.76%\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:      1.019 Validation Accuracy: 55.16%\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:      0.851 Validation Accuracy: 58.46%\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:      1.037 Validation Accuracy: 58.96%\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:      1.140 Validation Accuracy: 58.38%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:      1.117 Validation Accuracy: 58.32%\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:      0.911 Validation Accuracy: 60.98%\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:      0.879 Validation Accuracy: 56.36%\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:      1.286 Validation Accuracy: 53.96%\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:      1.034 Validation Accuracy: 61.50%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:      0.915 Validation Accuracy: 60.62%\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:      0.850 Validation Accuracy: 58.64%\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:      0.815 Validation Accuracy: 57.70%\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:      0.908 Validation Accuracy: 61.84%\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:      0.894 Validation Accuracy: 62.58%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:      0.782 Validation Accuracy: 65.96%\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:      0.771 Validation Accuracy: 58.70%\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:      0.601 Validation Accuracy: 63.28%\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:      0.810 Validation Accuracy: 63.18%\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:      0.833 Validation Accuracy: 64.44%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:      0.735 Validation Accuracy: 66.78%\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:      0.990 Validation Accuracy: 55.86%\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:      0.612 Validation Accuracy: 63.98%\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:      0.766 Validation Accuracy: 63.60%\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:      0.830 Validation Accuracy: 63.92%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:      0.715 Validation Accuracy: 68.04%\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:      0.626 Validation Accuracy: 64.72%\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:      0.684 Validation Accuracy: 62.86%\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:      0.833 Validation Accuracy: 61.90%\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:      0.733 Validation Accuracy: 65.92%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:      0.710 Validation Accuracy: 67.76%\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:      0.784 Validation Accuracy: 61.24%\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:      0.477 Validation Accuracy: 67.92%\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:      0.590 Validation Accuracy: 66.24%\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:      0.643 Validation Accuracy: 66.96%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:      0.727 Validation Accuracy: 67.04%\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:      0.554 Validation Accuracy: 66.20%\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:      0.435 Validation Accuracy: 67.26%\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:      0.669 Validation Accuracy: 65.42%\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:      0.603 Validation Accuracy: 67.76%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:      0.636 Validation Accuracy: 68.12%\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:      0.596 Validation Accuracy: 64.72%\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:      0.459 Validation Accuracy: 66.62%\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:      0.550 Validation Accuracy: 66.54%\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:      0.655 Validation Accuracy: 66.42%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:      0.495 Validation Accuracy: 70.24%\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:      0.502 Validation Accuracy: 63.00%\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:      0.405 Validation Accuracy: 67.24%\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:      0.612 Validation Accuracy: 66.94%\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:      0.647 Validation Accuracy: 67.24%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:      0.495 Validation Accuracy: 71.02%\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:      0.551 Validation Accuracy: 64.70%\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:      0.393 Validation Accuracy: 69.22%\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:      0.733 Validation Accuracy: 65.18%\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:      0.510 Validation Accuracy: 70.18%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:      0.469 Validation Accuracy: 71.82%\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:      0.683 Validation Accuracy: 61.58%\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:      0.396 Validation Accuracy: 67.24%\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:      0.495 Validation Accuracy: 67.48%\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:      0.554 Validation Accuracy: 68.34%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:      0.428 Validation Accuracy: 72.14%\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:      0.415 Validation Accuracy: 69.06%\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:      0.339 Validation Accuracy: 71.18%\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:      0.486 Validation Accuracy: 69.40%\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:      0.417 Validation Accuracy: 70.58%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:      0.398 Validation Accuracy: 72.54%\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:      0.406 Validation Accuracy: 71.56%\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:      0.337 Validation Accuracy: 67.94%\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:      0.640 Validation Accuracy: 63.94%\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:      0.762 Validation Accuracy: 63.94%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:      0.445 Validation Accuracy: 72.62%\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:      0.582 Validation Accuracy: 64.30%\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:      0.356 Validation Accuracy: 68.82%\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:      0.366 Validation Accuracy: 70.28%\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:      0.402 Validation Accuracy: 69.60%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:      0.371 Validation Accuracy: 74.16%\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:      0.396 Validation Accuracy: 70.08%\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:      0.437 Validation Accuracy: 64.34%\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:      0.321 Validation Accuracy: 71.66%\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:      0.515 Validation Accuracy: 69.42%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:      0.432 Validation Accuracy: 71.72%\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:      0.356 Validation Accuracy: 71.98%\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:      0.289 Validation Accuracy: 71.62%\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:      0.562 Validation Accuracy: 67.34%\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:      0.325 Validation Accuracy: 70.76%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:      0.497 Validation Accuracy: 71.16%\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:      0.413 Validation Accuracy: 66.64%\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:      0.227 Validation Accuracy: 71.24%\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:      0.406 Validation Accuracy: 72.04%\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:      0.288 Validation Accuracy: 72.18%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:      0.376 Validation Accuracy: 74.60%\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:      0.445 Validation Accuracy: 70.36%\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:      0.217 Validation Accuracy: 72.70%\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:      0.363 Validation Accuracy: 69.38%\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:      0.325 Validation Accuracy: 72.26%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:      0.360 Validation Accuracy: 74.28%\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:      0.545 Validation Accuracy: 65.96%\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:      0.190 Validation Accuracy: 72.12%\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:      0.254 Validation Accuracy: 72.94%\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:      0.357 Validation Accuracy: 71.66%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:      0.338 Validation Accuracy: 74.88%\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:      0.293 Validation Accuracy: 73.04%\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:      0.218 Validation Accuracy: 71.76%\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:      0.284 Validation Accuracy: 72.06%\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:      0.292 Validation Accuracy: 72.66%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:      0.352 Validation Accuracy: 74.80%\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:      0.359 Validation Accuracy: 70.56%\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:      0.159 Validation Accuracy: 73.30%\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:      0.279 Validation Accuracy: 72.46%\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:      0.234 Validation Accuracy: 74.38%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:      0.276 Validation Accuracy: 75.12%\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:      0.285 Validation Accuracy: 72.42%\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:      0.177 Validation Accuracy: 73.28%\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:      0.192 Validation Accuracy: 75.18%\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:      0.243 Validation Accuracy: 72.48%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:      0.243 Validation Accuracy: 75.94%\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:      0.297 Validation Accuracy: 71.64%\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:      0.210 Validation Accuracy: 73.84%\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:      0.241 Validation Accuracy: 72.02%\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:      0.271 Validation Accuracy: 69.56%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:      0.248 Validation Accuracy: 76.02%\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:      0.267 Validation Accuracy: 75.24%\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:      0.213 Validation Accuracy: 73.70%\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:      0.215 Validation Accuracy: 72.88%\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:      0.234 Validation Accuracy: 72.14%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:      0.311 Validation Accuracy: 75.56%\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:      0.288 Validation Accuracy: 73.78%\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:      0.151 Validation Accuracy: 74.16%\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:      0.185 Validation Accuracy: 74.38%\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:      0.161 Validation Accuracy: 75.52%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:      0.254 Validation Accuracy: 76.26%\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:      0.251 Validation Accuracy: 73.98%\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:      0.152 Validation Accuracy: 74.34%\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:      0.181 Validation Accuracy: 74.18%\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:      0.138 Validation Accuracy: 75.90%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:      0.250 Validation Accuracy: 76.94%\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:      0.182 Validation Accuracy: 76.10%\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:      0.172 Validation Accuracy: 74.72%\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:      0.188 Validation Accuracy: 75.88%\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:      0.169 Validation Accuracy: 76.10%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:      0.238 Validation Accuracy: 76.50%\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:      0.247 Validation Accuracy: 74.38%\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:      0.148 Validation Accuracy: 73.70%\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:      0.139 Validation Accuracy: 75.42%\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:      0.182 Validation Accuracy: 73.38%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:      0.264 Validation Accuracy: 75.64%\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:      0.240 Validation Accuracy: 75.52%\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:      0.179 Validation Accuracy: 73.80%\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:      0.135 Validation Accuracy: 75.48%\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:      0.199 Validation Accuracy: 73.12%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:      0.208 Validation Accuracy: 77.32%\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:      0.231 Validation Accuracy: 75.60%\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:      0.165 Validation Accuracy: 74.12%\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:      0.193 Validation Accuracy: 73.00%\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:      0.183 Validation Accuracy: 73.06%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:      0.211 Validation Accuracy: 76.00%\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:      0.150 Validation Accuracy: 77.64%\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:      0.134 Validation Accuracy: 73.64%\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:      0.181 Validation Accuracy: 73.60%\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:      0.111 Validation Accuracy: 76.34%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:      0.254 Validation Accuracy: 75.38%\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:      0.147 Validation Accuracy: 77.42%\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:      0.111 Validation Accuracy: 75.86%\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:      0.119 Validation Accuracy: 75.44%\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:      0.112 Validation Accuracy: 74.64%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:      0.223 Validation Accuracy: 75.88%\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:      0.167 Validation Accuracy: 76.88%\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:      0.089 Validation Accuracy: 76.42%\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:      0.164 Validation Accuracy: 74.94%\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:      0.130 Validation Accuracy: 73.86%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:      0.213 Validation Accuracy: 76.88%\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:      0.120 Validation Accuracy: 76.26%\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:      0.101 Validation Accuracy: 75.64%\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:      0.106 Validation Accuracy: 77.52%\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:      0.092 Validation Accuracy: 75.46%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:      0.189 Validation Accuracy: 76.88%\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:      0.156 Validation Accuracy: 76.46%\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:      0.086 Validation Accuracy: 75.68%\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:      0.109 Validation Accuracy: 78.20%\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:      0.108 Validation Accuracy: 75.84%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:      0.163 Validation Accuracy: 77.74%\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:      0.136 Validation Accuracy: 78.40%\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:      0.066 Validation Accuracy: 76.68%\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:      0.089 Validation Accuracy: 77.32%\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:      0.112 Validation Accuracy: 72.70%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:      0.203 Validation Accuracy: 76.42%\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:      0.216 Validation Accuracy: 73.98%\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:      0.072 Validation Accuracy: 75.96%\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:      0.071 Validation Accuracy: 76.66%\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:      0.086 Validation Accuracy: 76.12%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:      0.238 Validation Accuracy: 74.88%\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:      0.162 Validation Accuracy: 76.76%\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:      0.056 Validation Accuracy: 75.62%\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:      0.078 Validation Accuracy: 76.68%\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:      0.100 Validation Accuracy: 75.26%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:      0.128 Validation Accuracy: 77.44%\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:      0.122 Validation Accuracy: 78.40%\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:      0.075 Validation Accuracy: 75.16%\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:      0.091 Validation Accuracy: 77.00%\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:      0.077 Validation Accuracy: 77.10%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:      0.165 Validation Accuracy: 75.18%\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:      0.286 Validation Accuracy: 74.50%\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:      0.070 Validation Accuracy: 76.38%\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:      0.081 Validation Accuracy: 77.86%\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:      0.092 Validation Accuracy: 76.56%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:      0.124 Validation Accuracy: 77.58%\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:      0.105 Validation Accuracy: 77.12%\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:      0.114 Validation Accuracy: 73.38%\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:      0.065 Validation Accuracy: 77.96%\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:      0.099 Validation Accuracy: 75.42%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:      0.104 Validation Accuracy: 77.12%\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:      0.112 Validation Accuracy: 76.96%\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:      0.057 Validation Accuracy: 76.48%\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:      0.076 Validation Accuracy: 78.40%\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:      0.054 Validation Accuracy: 75.78%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:      0.103 Validation Accuracy: 78.38%\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:      0.124 Validation Accuracy: 77.36%\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:      0.044 Validation Accuracy: 76.28%\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:      0.075 Validation Accuracy: 78.04%\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:      0.098 Validation Accuracy: 74.96%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:      0.129 Validation Accuracy: 77.42%\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:      0.113 Validation Accuracy: 76.80%\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:      0.061 Validation Accuracy: 76.00%\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:      0.077 Validation Accuracy: 77.56%\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:      0.036 Validation Accuracy: 78.26%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:      0.113 Validation Accuracy: 77.06%\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:      0.092 Validation Accuracy: 78.66%\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:      0.056 Validation Accuracy: 76.98%\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:      0.054 Validation Accuracy: 79.02%\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:      0.055 Validation Accuracy: 77.76%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:      0.127 Validation Accuracy: 76.88%\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:      0.130 Validation Accuracy: 77.26%\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:      0.047 Validation Accuracy: 77.54%\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:      0.074 Validation Accuracy: 78.48%\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:      0.044 Validation Accuracy: 77.04%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:      0.099 Validation Accuracy: 78.26%\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:      0.125 Validation Accuracy: 75.32%\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:      0.068 Validation Accuracy: 76.56%\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:      0.061 Validation Accuracy: 79.10%\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:      0.045 Validation Accuracy: 77.02%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:      0.076 Validation Accuracy: 77.74%\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:      0.109 Validation Accuracy: 78.20%\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:      0.030 Validation Accuracy: 78.86%\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:      0.067 Validation Accuracy: 78.22%\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:      0.033 Validation Accuracy: 79.36%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:      0.066 Validation Accuracy: 79.98%\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:      0.121 Validation Accuracy: 77.38%\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:      0.045 Validation Accuracy: 77.28%\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:      0.056 Validation Accuracy: 79.10%\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:      0.045 Validation Accuracy: 75.66%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:      0.078 Validation Accuracy: 77.82%\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:      0.068 Validation Accuracy: 79.36%\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:      0.044 Validation Accuracy: 77.96%\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:      0.062 Validation Accuracy: 77.38%\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:      0.051 Validation Accuracy: 76.74%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:      0.072 Validation Accuracy: 79.24%\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:      0.105 Validation Accuracy: 76.98%\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:      0.042 Validation Accuracy: 77.78%\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:      0.050 Validation Accuracy: 78.26%\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:      0.036 Validation Accuracy: 75.96%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:      0.070 Validation Accuracy: 77.94%\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:      0.060 Validation Accuracy: 78.84%\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:      0.020 Validation Accuracy: 77.88%\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:      0.040 Validation Accuracy: 79.12%\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:      0.027 Validation Accuracy: 78.44%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:      0.071 Validation Accuracy: 78.24%\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:      0.070 Validation Accuracy: 78.80%\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:      0.021 Validation Accuracy: 78.18%\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:      0.058 Validation Accuracy: 78.00%\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:      0.022 Validation Accuracy: 79.12%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:      0.065 Validation Accuracy: 78.92%\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:      0.097 Validation Accuracy: 77.08%\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:      0.019 Validation Accuracy: 78.76%\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:      0.064 Validation Accuracy: 78.02%\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:      0.028 Validation Accuracy: 78.58%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:      0.057 Validation Accuracy: 79.52%\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:      0.076 Validation Accuracy: 79.02%\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:      0.031 Validation Accuracy: 78.24%\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:      0.037 Validation Accuracy: 79.02%\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:      0.024 Validation Accuracy: 78.60%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:      0.048 Validation Accuracy: 78.52%\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:      0.046 Validation Accuracy: 79.68%\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:      0.018 Validation Accuracy: 78.06%\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:      0.055 Validation Accuracy: 78.14%\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:      0.026 Validation Accuracy: 78.88%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:      0.057 Validation Accuracy: 77.90%\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:      0.057 Validation Accuracy: 78.62%\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:      0.018 Validation Accuracy: 79.58%\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:      0.073 Validation Accuracy: 76.98%\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:      0.021 Validation Accuracy: 78.14%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:      0.038 Validation Accuracy: 80.34%\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:      0.034 Validation Accuracy: 79.40%\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:      0.030 Validation Accuracy: 78.24%\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:      0.051 Validation Accuracy: 77.90%\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:      0.047 Validation Accuracy: 77.46%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:      0.037 Validation Accuracy: 78.84%\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:      0.050 Validation Accuracy: 78.10%\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:      0.016 Validation Accuracy: 79.28%\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:      0.037 Validation Accuracy: 79.46%\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:      0.026 Validation Accuracy: 79.18%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:      0.043 Validation Accuracy: 79.26%\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:      0.051 Validation Accuracy: 79.30%\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:      0.019 Validation Accuracy: 78.74%\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:      0.025 Validation Accuracy: 79.34%\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:      0.027 Validation Accuracy: 77.46%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:      0.037 Validation Accuracy: 79.36%\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:      0.057 Validation Accuracy: 79.48%\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:      0.025 Validation Accuracy: 78.92%\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:      0.044 Validation Accuracy: 77.76%\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:      0.028 Validation Accuracy: 78.32%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:      0.034 Validation Accuracy: 79.52%\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:      0.038 Validation Accuracy: 80.02%\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:      0.010 Validation Accuracy: 79.64%\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:      0.030 Validation Accuracy: 79.98%\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:      0.030 Validation Accuracy: 77.10%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:      0.037 Validation Accuracy: 79.78%\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:      0.043 Validation Accuracy: 79.08%\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:      0.017 Validation Accuracy: 79.70%\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:      0.031 Validation Accuracy: 78.92%\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:      0.027 Validation Accuracy: 79.16%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:      0.050 Validation Accuracy: 79.04%\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:      0.059 Validation Accuracy: 78.32%\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:      0.009 Validation Accuracy: 79.60%\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:      0.035 Validation Accuracy: 80.14%\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:      0.028 Validation Accuracy: 78.32%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:      0.031 Validation Accuracy: 79.20%\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:      0.110 Validation Accuracy: 77.58%\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:      0.009 Validation Accuracy: 78.78%\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:      0.030 Validation Accuracy: 79.64%\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:      0.013 Validation Accuracy: 80.22%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:      0.050 Validation Accuracy: 80.00%\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:      0.054 Validation Accuracy: 76.86%\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:      0.013 Validation Accuracy: 79.16%\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:      0.040 Validation Accuracy: 78.34%\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:      0.010 Validation Accuracy: 79.00%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:      0.024 Validation Accuracy: 79.64%\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:      0.068 Validation Accuracy: 78.64%\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:      0.012 Validation Accuracy: 78.38%\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:      0.033 Validation Accuracy: 79.92%\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:      0.018 Validation Accuracy: 77.70%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:      0.027 Validation Accuracy: 80.06%\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:      0.031 Validation Accuracy: 79.44%\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:      0.015 Validation Accuracy: 77.44%\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:      0.016 Validation Accuracy: 80.24%\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:      0.009 Validation Accuracy: 79.92%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:      0.037 Validation Accuracy: 79.66%\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:      0.059 Validation Accuracy: 78.38%\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:      0.017 Validation Accuracy: 79.20%\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:      0.029 Validation Accuracy: 78.48%\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:      0.010 Validation Accuracy: 79.72%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:      0.046 Validation Accuracy: 79.78%\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:      0.048 Validation Accuracy: 78.34%\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:      0.007 Validation Accuracy: 79.58%\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:      0.015 Validation Accuracy: 80.34%\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:      0.025 Validation Accuracy: 78.50%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:      0.032 Validation Accuracy: 81.44%\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:      0.037 Validation Accuracy: 79.12%\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:      0.011 Validation Accuracy: 78.42%\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:      0.028 Validation Accuracy: 79.42%\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:      0.014 Validation Accuracy: 78.90%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:      0.050 Validation Accuracy: 78.74%\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:      0.034 Validation Accuracy: 78.44%\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:      0.015 Validation Accuracy: 75.82%\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:      0.022 Validation Accuracy: 79.74%\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:      0.013 Validation Accuracy: 79.42%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:      0.019 Validation Accuracy: 80.12%\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:      0.028 Validation Accuracy: 80.18%\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:      0.013 Validation Accuracy: 78.58%\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:      0.020 Validation Accuracy: 79.14%\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:      0.011 Validation Accuracy: 79.04%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:      0.016 Validation Accuracy: 79.42%\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:      0.097 Validation Accuracy: 76.48%\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:      0.008 Validation Accuracy: 78.76%\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:      0.028 Validation Accuracy: 79.32%\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:      0.011 Validation Accuracy: 79.20%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:      0.021 Validation Accuracy: 80.10%\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:      0.030 Validation Accuracy: 80.02%\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:      0.007 Validation Accuracy: 79.72%\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:      0.016 Validation Accuracy: 79.74%\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:      0.011 Validation Accuracy: 79.74%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:      0.027 Validation Accuracy: 79.76%\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:      0.057 Validation Accuracy: 78.00%\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:      0.011 Validation Accuracy: 79.08%\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:      0.026 Validation Accuracy: 80.16%\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 80.70%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:      0.027 Validation Accuracy: 79.94%\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:      0.035 Validation Accuracy: 80.78%\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:      0.008 Validation Accuracy: 79.82%\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:      0.022 Validation Accuracy: 80.46%\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 79.12%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:      0.049 Validation Accuracy: 78.88%\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:      0.057 Validation Accuracy: 79.90%\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:      0.013 Validation Accuracy: 79.58%\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:      0.016 Validation Accuracy: 79.26%\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 79.58%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:      0.025 Validation Accuracy: 80.84%\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:      0.033 Validation Accuracy: 79.08%\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:      0.008 Validation Accuracy: 78.74%\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:      0.010 Validation Accuracy: 80.40%\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:      0.007 Validation Accuracy: 79.72%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:      0.015 Validation Accuracy: 80.76%\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:      0.047 Validation Accuracy: 79.30%\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:      0.009 Validation Accuracy: 79.58%\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:      0.017 Validation Accuracy: 80.90%\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:      0.005 Validation Accuracy: 80.46%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:      0.038 Validation Accuracy: 79.20%\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:      0.032 Validation Accuracy: 78.32%\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:      0.012 Validation Accuracy: 80.24%\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:      0.012 Validation Accuracy: 80.16%\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 79.70%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:      0.023 Validation Accuracy: 79.66%\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:      0.052 Validation Accuracy: 79.48%\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:      0.014 Validation Accuracy: 80.16%\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:      0.031 Validation Accuracy: 78.62%\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 78.58%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:      0.041 Validation Accuracy: 79.64%\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:      0.061 Validation Accuracy: 77.98%\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:      0.011 Validation Accuracy: 78.92%\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:      0.015 Validation Accuracy: 80.16%\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:      0.009 Validation Accuracy: 79.82%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:      0.019 Validation Accuracy: 79.86%\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:      0.065 Validation Accuracy: 77.62%\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 79.80%\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:      0.031 Validation Accuracy: 77.70%\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:      0.011 Validation Accuracy: 79.20%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:      0.016 Validation Accuracy: 80.84%\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 80.64%\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:      0.008 Validation Accuracy: 80.16%\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:      0.006 Validation Accuracy: 80.48%\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:      0.006 Validation Accuracy: 80.36%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:      0.014 Validation Accuracy: 81.48%\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:      0.040 Validation Accuracy: 78.48%\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 80.82%\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:      0.008 Validation Accuracy: 80.88%\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:      0.009 Validation Accuracy: 80.00%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:      0.022 Validation Accuracy: 80.86%\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:      0.025 Validation Accuracy: 79.30%\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 81.30%\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:      0.008 Validation Accuracy: 80.34%\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:      0.005 Validation Accuracy: 79.36%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:      0.023 Validation Accuracy: 80.16%\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:      0.037 Validation Accuracy: 79.10%\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:      0.004 Validation Accuracy: 80.20%\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:      0.017 Validation Accuracy: 80.32%\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:      0.007 Validation Accuracy: 78.82%\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:      0.023 Validation Accuracy: 80.20%\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:      0.070 Validation Accuracy: 78.08%\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 80.88%\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:      0.009 Validation Accuracy: 81.58%\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 78.30%\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:      0.025 Validation Accuracy: 81.06%\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:      0.019 Validation Accuracy: 80.62%\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 79.78%\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 81.02%\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:      0.005 Validation Accuracy: 79.32%\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:      0.014 Validation Accuracy: 80.80%\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:      0.022 Validation Accuracy: 79.86%\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:      0.012 Validation Accuracy: 81.10%\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:      0.006 Validation Accuracy: 80.68%\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 80.28%\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:      0.013 Validation Accuracy: 80.94%\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 78.44%\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 79.48%\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:      0.013 Validation Accuracy: 80.10%\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.94%\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:      0.009 Validation Accuracy: 81.24%\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:      0.023 Validation Accuracy: 79.62%\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 80.08%\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:      0.011 Validation Accuracy: 80.38%\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:      0.009 Validation Accuracy: 80.28%\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:      0.032 Validation Accuracy: 80.54%\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:      0.023 Validation Accuracy: 79.08%\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 80.86%\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:      0.009 Validation Accuracy: 81.06%\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:      0.009 Validation Accuracy: 80.56%\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 80.98%\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:      0.052 Validation Accuracy: 77.40%\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 80.04%\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 80.62%\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:      0.021 Validation Accuracy: 78.62%\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:      0.014 Validation Accuracy: 80.42%\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:      0.017 Validation Accuracy: 78.48%\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 79.66%\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 81.38%\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 80.48%\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:      0.009 Validation Accuracy: 80.68%\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 79.98%\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 80.30%\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 81.02%\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 79.82%\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 80.92%\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:      0.012 Validation Accuracy: 81.14%\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 80.02%\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:      0.016 Validation Accuracy: 81.08%\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 80.86%\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 80.86%\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:      0.022 Validation Accuracy: 79.68%\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 80.72%\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:      0.014 Validation Accuracy: 80.24%\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:      0.013 Validation Accuracy: 79.18%\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:      0.014 Validation Accuracy: 79.70%\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:      0.016 Validation Accuracy: 79.42%\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 79.42%\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:      0.006 Validation Accuracy: 81.32%\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 80.16%\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 81.82%\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:      0.031 Validation Accuracy: 78.22%\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 80.98%\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 80.48%\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:      0.012 Validation Accuracy: 79.88%\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:      0.012 Validation Accuracy: 80.62%\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:      0.008 Validation Accuracy: 80.38%\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 78.82%\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:      0.010 Validation Accuracy: 80.38%\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 79.76%\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 80.70%\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:      0.013 Validation Accuracy: 80.22%\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 80.50%\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:      0.012 Validation Accuracy: 80.50%\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 80.50%\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:      0.011 Validation Accuracy: 80.48%\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:      0.013 Validation Accuracy: 78.84%\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:      0.009 Validation Accuracy: 79.48%\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:      0.010 Validation Accuracy: 81.40%\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:      0.012 Validation Accuracy: 81.08%\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 80.80%\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:      0.012 Validation Accuracy: 79.98%\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 81.14%\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:      0.008 Validation Accuracy: 79.98%\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 79.94%\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 80.80%\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:      0.030 Validation Accuracy: 77.82%\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 80.86%\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 80.82%\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 79.50%\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 80.90%\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:      0.008 Validation Accuracy: 80.22%\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 79.94%\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 80.82%\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:      0.007 Validation Accuracy: 79.58%\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 81.32%\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:      0.009 Validation Accuracy: 79.76%\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 80.88%\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 80.06%\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 80.94%\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:      0.014 Validation Accuracy: 80.08%\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:      0.047 Validation Accuracy: 78.12%\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 81.08%\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.10%\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 80.76%\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:      0.012 Validation Accuracy: 79.84%\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:      0.024 Validation Accuracy: 78.18%\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 80.96%\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.38%\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 80.86%\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:      0.016 Validation Accuracy: 81.92%\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:      0.028 Validation Accuracy: 80.60%\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 81.30%\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.30%\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:      0.006 Validation Accuracy: 80.34%\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 80.40%\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:      0.024 Validation Accuracy: 80.42%\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:      0.008 Validation Accuracy: 79.26%\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 80.88%\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.78%\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:      0.011 Validation Accuracy: 81.36%\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:      0.059 Validation Accuracy: 78.08%\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 80.24%\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.18%\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.46%\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 80.98%\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 80.42%\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 80.92%\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:      0.006 Validation Accuracy: 79.54%\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:      0.008 Validation Accuracy: 80.72%\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 81.50%\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:      0.008 Validation Accuracy: 80.38%\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.64%\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 81.34%\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:      0.010 Validation Accuracy: 79.24%\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.64%\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:      0.025 Validation Accuracy: 78.76%\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.62%\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:      0.012 Validation Accuracy: 81.26%\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.96%\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.60%\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:      0.049 Validation Accuracy: 77.88%\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 80.62%\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.84%\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:      0.007 Validation Accuracy: 78.28%\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 81.64%\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:      0.033 Validation Accuracy: 78.56%\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 80.78%\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.24%\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.56%\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.02%\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 80.66%\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:      0.003 Validation Accuracy: 79.56%\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 80.28%\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:      0.009 Validation Accuracy: 81.14%\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.78%\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:      0.007 Validation Accuracy: 81.14%\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 80.36%\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.56%\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.70%\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:      0.012 Validation Accuracy: 81.38%\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:      0.013 Validation Accuracy: 78.64%\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:      0.004 Validation Accuracy: 80.98%\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.52%\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.16%\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:      0.012 Validation Accuracy: 80.38%\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 78.34%\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:      0.004 Validation Accuracy: 80.92%\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 80.86%\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.64%\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:      0.009 Validation Accuracy: 81.06%\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:      0.018 Validation Accuracy: 79.12%\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:      0.004 Validation Accuracy: 81.04%\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.76%\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:      0.007 Validation Accuracy: 77.86%\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.66%\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:      0.007 Validation Accuracy: 81.44%\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.14%\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.58%\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.80%\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:      0.016 Validation Accuracy: 81.40%\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:      0.023 Validation Accuracy: 79.08%\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 80.84%\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 82.08%\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.58%\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:      0.018 Validation Accuracy: 80.52%\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:      0.011 Validation Accuracy: 79.38%\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.64%\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.70%\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.22%\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.66%\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:      0.016 Validation Accuracy: 79.26%\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 81.58%\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.22%\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:      0.011 Validation Accuracy: 79.42%\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.06%\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:      0.009 Validation Accuracy: 79.76%\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 80.88%\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.60%\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.18%\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:      0.011 Validation Accuracy: 81.10%\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:      0.024 Validation Accuracy: 78.64%\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:      0.004 Validation Accuracy: 81.34%\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 80.82%\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.66%\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.10%\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:      0.006 Validation Accuracy: 81.22%\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.62%\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 82.16%\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.96%\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 81.38%\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 81.48%\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.56%\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:      0.006 Validation Accuracy: 81.50%\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.96%\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 81.78%\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:      0.010 Validation Accuracy: 80.40%\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.80%\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:      0.007 Validation Accuracy: 81.20%\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:      0.012 Validation Accuracy: 80.24%\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.72%\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:      0.010 Validation Accuracy: 79.42%\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.44%\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.82%\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.86%\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.36%\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:      0.009 Validation Accuracy: 79.80%\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 81.28%\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.48%\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 80.66%\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.24%\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 80.50%\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.06%\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 82.06%\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.76%\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.76%\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:      0.006 Validation Accuracy: 80.28%\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.82%\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:      0.008 Validation Accuracy: 81.70%\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.34%\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 79.92%\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.52%\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.22%\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:      0.010 Validation Accuracy: 80.70%\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 80.92%\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.10%\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.56%\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.94%\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.68%\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 79.56%\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 80.90%\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 81.34%\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.20%\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.54%\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.36%\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.66%\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 81.84%\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 82.10%\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.44%\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.02%\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.74%\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:      0.009 Validation Accuracy: 81.44%\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.70%\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.84%\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.26%\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:      0.010 Validation Accuracy: 81.58%\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.88%\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:      0.005 Validation Accuracy: 81.12%\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 80.72%\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.22%\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 81.22%\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.04%\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.48%\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.36%\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.78%\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 79.80%\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.26%\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.78%\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.38%\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.60%\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.34%\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.86%\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.90%\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 82.48%\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.26%\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 81.88%\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:      0.014 Validation Accuracy: 81.84%\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.42%\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 82.06%\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.12%\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 82.30%\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.12%\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.28%\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 82.00%\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.14%\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 82.22%\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 81.60%\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 82.56%\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.84%\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.62%\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 82.20%\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.78%\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.58%\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.92%\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.00%\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.70%\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:      0.010 Validation Accuracy: 81.64%\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.98%\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 81.78%\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.28%\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.40%\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 82.06%\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.70%\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.78%\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.54%\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 82.22%\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 81.76%\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.10%\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.12%\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 79.78%\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.16%\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:      0.008 Validation Accuracy: 79.62%\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.98%\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.76%\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 81.66%\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.98%\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.36%\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.62%\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.26%\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.50%\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 81.76%\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 81.16%\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.36%\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.40%\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.00%\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.82%\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:      0.007 Validation Accuracy: 80.44%\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.02%\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 82.08%\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 80.82%\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 80.84%\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.10%\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.62%\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.88%\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 81.58%\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 81.72%\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 81.14%\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.06%\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.46%\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.40%\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.38%\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 82.14%\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.84%\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.88%\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 82.20%\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.82%\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.82%\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.54%\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:      0.005 Validation Accuracy: 81.74%\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 80.74%\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 82.16%\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:      0.005 Validation Accuracy: 81.72%\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.64%\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 81.66%\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.70%\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 82.44%\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 82.36%\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 82.28%\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 82.32%\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 81.88%\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 82.18%\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.80%\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.32%\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 82.22%\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.24%\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:      0.009 Validation Accuracy: 81.76%\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 82.34%\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.72%\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 82.30%\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.78%\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 82.28%\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.36%\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.46%\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 82.36%\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.84%\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.66%\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 81.80%\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.86%\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.44%\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:      0.004 Validation Accuracy: 81.34%\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 82.42%\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:      0.008 Validation Accuracy: 81.44%\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:      0.004 Validation Accuracy: 81.84%\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.48%\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 81.20%\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:      0.008 Validation Accuracy: 81.82%\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.36%\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 82.48%\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.98%\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:      0.005 Validation Accuracy: 80.38%\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.70%\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 82.16%\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.30%\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 82.24%\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.82%\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 81.86%\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 80.54%\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 82.14%\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.92%\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.14%\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 82.80%\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 82.10%\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 81.86%\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 82.08%\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 82.16%\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.88%\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 81.28%\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.38%\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 81.34%\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.40%\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 82.04%\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 81.48%\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 82.04%\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 81.56%\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.48%\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.50%\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 81.12%\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 82.36%\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 80.72%\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:      0.003 Validation Accuracy: 81.18%\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 81.98%\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.90%\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 82.38%\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 82.00%\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.96%\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 82.38%\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 82.00%\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 82.60%\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.92%\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.86%\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 82.44%\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.26%\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.94%\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.78%\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.78%\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 82.46%\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 81.74%\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 82.06%\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 82.08%\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.48%\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:      0.007 Validation Accuracy: 81.66%\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 82.16%\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.76%\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 82.04%\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.06%\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:      0.006 Validation Accuracy: 82.16%\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:      0.001 Validation Accuracy: 81.78%\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.36%\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.98%\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 80.76%\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 81.58%\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.30%\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 81.84%\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.72%\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.70%\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:      0.003 Validation Accuracy: 81.94%\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:      0.008 Validation Accuracy: 80.90%\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 81.14%\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 82.20%\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.46%\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 82.06%\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:      0.006 Validation Accuracy: 81.04%\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 82.72%\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:      0.004 Validation Accuracy: 81.72%\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.32%\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.50%\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 82.26%\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.64%\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 81.84%\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.32%\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 82.20%\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:      0.004 Validation Accuracy: 81.94%\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.92%\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.74%\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.98%\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:      0.004 Validation Accuracy: 81.94%\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 81.42%\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:      0.000 Validation Accuracy: 82.02%\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:      0.003 Validation Accuracy: 81.80%\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 81.80%\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:      0.002 Validation Accuracy: 82.62%\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:      0.002 Validation Accuracy: 81.62%\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:      0.001 Validation Accuracy: 81.32%\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:      0.001 Validation Accuracy: 82.22%\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:      0.001 Validation Accuracy: 80.64%\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:      0.005 Validation Accuracy: 82.00%\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:      0.003 Validation Accuracy: 82.00%\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:      0.002 Validation Accuracy: 81.30%\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:      0.002 Validation Accuracy: 81.52%\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:      0.002 Validation Accuracy: 81.96%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.818359375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF/SVmt7aox8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1efc4ffb9b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
